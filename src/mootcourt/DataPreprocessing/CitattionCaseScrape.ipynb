{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_case_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the relevant section containing case details (this may vary based on website structure)\n",
    "    cases = []\n",
    "    \n",
    "    # Example: Assuming cases are listed in <div class=\"case\"> tags (you need to inspect the actual HTML)\n",
    "    for case_div in soup.find_all('div', class_='case'):\n",
    "        case_name = case_div.find('h2').text.strip()  # Adjust based on actual structure\n",
    "        citation = case_div.find('span', class_='citation').text.strip()  # Adjust based on actual structure\n",
    "        year = case_div.find('span', class_='year').text.strip()  # Adjust based on actual structure\n",
    "        facts = case_div.find('p', class_='facts').text.strip()  # Adjust based on actual structure\n",
    "        issues = case_div.find('p', class_='issues').text.strip()  # Adjust based on actual structure\n",
    "        judgment_summary = case_div.find('p', class_='summary').text.strip()  # Adjust based on actual structure\n",
    "        \n",
    "        cases.append({\n",
    "            'case_name': case_name,\n",
    "            'citation': citation,\n",
    "            'year': year,\n",
    "            'facts': facts,\n",
    "            'issues': issues,\n",
    "            'judgment_summary': judgment_summary\n",
    "        })\n",
    "    \n",
    "    return cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as constitutional_cases_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    url = \"https://www.indiankanoon.org/search/?formInput=constitution\"  # Example URL (modify as needed)\n",
    "    all_cases = scrape_case_data(url)\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(all_cases)\n",
    "    df.to_csv('constitutional_cases_dataset.csv', index=False)\n",
    "    print(\"Dataset saved as constitutional_cases_dataset.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [401]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def get_cases(keyword, max_cases=10):\n",
    "    url = f\"https://api.indiankanoon.org/search/?formInput={keyword}\"\n",
    "    response = requests.get(url)\n",
    "    # Parse response to extract case details\n",
    "    return response\n",
    "\n",
    "\n",
    "print(get_cases(\"Article 14\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: API returned status code 405\n",
      "Saved 0 cases to CSV\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_cases(keyword=\"constitution\", max_cases=200):\n",
    "    API_KEY = \"eb9ac60185dc449a177c16a3f881476b16999a01\"  # Replace with your API key\n",
    "    headers = {\"Authorization\": f\"Token {API_KEY}\"}\n",
    "    \n",
    "    cases = []\n",
    "    page = 1\n",
    "    \n",
    "    try:\n",
    "        while len(cases) < max_cases:\n",
    "            url = f\"https://api.indiankanoon.org/search/?formInput={keyword}&pn={page}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error: API returned status code {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "            data = response.json()\n",
    "            \n",
    "            if not data.get(\"results\"):\n",
    "                print(\"No more results found\")\n",
    "                break\n",
    "                \n",
    "            for result in data[\"results\"]:\n",
    "                cases.append({\n",
    "                    \"case_name\": result.get(\"title\", \"N/A\"),\n",
    "                    \"citation\": result.get(\"citation\", \"N/A\"),\n",
    "                    \"year\": result.get(\"date\", \"N/A\").split(\"-\")[0] if result.get(\"date\") else \"N/A\",\n",
    "                    \"judgment_summary\": result.get(\"doc\", \"N/A\")[:200] + \"...\",  # First 200 characters\n",
    "                    \"url\": f\"https://indiankanoon.org/doc/{result['tid']}/\"\n",
    "                })\n",
    "                \n",
    "                if len(cases) >= max_cases:\n",
    "                    break\n",
    "                    \n",
    "            page += 1\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        return pd.DataFrame(cases)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return pd.DataFrame(cases)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df = get_cases(keyword=\"article 14\", max_cases=200)\n",
    "    df.to_csv(\"indian_constitutional_cases.csv\", index=False)\n",
    "    print(f\"Saved {len(df)} cases to CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_cases(keyword=\"constitution\", max_cases=200):\n",
    "    API_KEY = \"eb9ac60185dc449a177c16a3f881476b16999a01\"  # Replace with your actual API key\n",
    "    headers = {\"Authorization\": f\"Token {API_KEY}\"}  # Using Token Auth\n",
    "    \n",
    "    cases = []\n",
    "    page = 0  # pagenum starts at 0\n",
    "    \n",
    "    try:\n",
    "        while len(cases) < max_cases:\n",
    "            url = f\"https://api.indiankanoon.org/search/?formInput={keyword}&pagenum={page}\"  # SEARCH endpoint\n",
    "            response = requests.post(url, headers=headers)  # POST request\n",
    "            \n",
    "            if response.status_code == 404:\n",
    "                print(\"Error 404: Check URL, parameters, or API endpoint.\")\n",
    "                break\n",
    "            \n",
    "            if response.status_code == 403:\n",
    "                print(\"Error 403: Authentication failed. Check API key format or status.\")\n",
    "                break\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error {response.status_code}: {response.text}\")\n",
    "                break\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if not data.get(\"docs\"):  # \"docs\" instead of \"results\"\n",
    "                print(\"No more results found.\")\n",
    "                break\n",
    "            \n",
    "            for doc in data[\"docs\"]:  # Use \"docs\" key\n",
    "                cases.append({\n",
    "                    \"case_name\": doc.get(\"title\", \"N/A\"),\n",
    "                    \"citation\": doc.get(\"citation\", \"N/A\"),\n",
    "                    \"year\": doc.get(\"publishdate\", \"N/A\").split(\"-\")[0] if doc.get(\"publishdate\") else \"N/A\",\n",
    "                    \"url\": f\"https://indiankanoon.org/doc/{doc['tid']}/\"\n",
    "                })\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(2)  # Rate limiting\n",
    "            \n",
    "        return pd.DataFrame(cases)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        return pd.DataFrame(cases)\n",
    "\n",
    "# Example\n",
    "df = get_cases(keyword=\"constitution\")\n",
    "df.to_csv(\"cases.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'examples'}\n",
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [Metadata(Legal Analysis using IPC Dataset)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ipc_sections.csv\n",
      "\n",
      "Using record set: ipc_sections.csv\n",
      "                        ipc_sections.csv/Description  \\\n",
      "0  b'Description of IPC Section 140\\nAccording to...   \n",
      "1  b'Description of IPC Section 127\\nAccording to...   \n",
      "2  b'Description of IPC Section 128\\nAccording to...   \n",
      "3  b'Description of IPC Section 129\\nAccording to...   \n",
      "4  b'Description of IPC Section 130\\nAccording to...   \n",
      "\n",
      "                            ipc_sections.csv/Offense  \\\n",
      "0  b'Wearing the dress or carrying any token used...   \n",
      "1  b'Receiving property taken by war or depredati...   \n",
      "2  b'Public servant voluntarily allowing prisoner...   \n",
      "3  b'Public servant negligently suffering prisone...   \n",
      "4  b'Aiding escape of, rescuing or harbouring, su...   \n",
      "\n",
      "                   ipc_sections.csv/Punishment ipc_sections.csv/Section  \n",
      "0                  b'3 Months or Fine or Both'               b'IPC_140'  \n",
      "1   b'7 Years + Fine + forfeiture of property'               b'IPC_127'  \n",
      "2  b'Imprisonment for Life or 10 Years + Fine'               b'IPC_128'  \n",
      "3        b'Simple Imprisonment 3 Years + Fine'               b'IPC_129'  \n",
      "4  b'Imprisonment for Life or 10 Years + Fine'               b'IPC_130'  \n",
      "\n",
      "Saved dataset to 'ipc_sections.csv.csv'\n"
     ]
    }
   ],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Croissant Dataset\n",
    "croissant_dataset = mlc.Dataset('https://www.kaggle.com/datasets/kanishhkaa/legal-analysis-using-ipc-dataset/croissant/download')\n",
    "\n",
    "# List all record sets\n",
    "record_sets = croissant_dataset.metadata.record_sets\n",
    "for i, rs in enumerate(record_sets):\n",
    "    print(f\"{i}: {rs.name}\")\n",
    "\n",
    "# Assuming you're using the first record set\n",
    "record_set = record_sets[0]\n",
    "print(f\"\\nUsing record set: {record_set.name}\")\n",
    "\n",
    "# Convert records to DataFrame\n",
    "records = croissant_dataset.records(record_set=record_set.uuid)\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Preview the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = f\"{record_set.name.replace(' ', '_')}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nSaved dataset to '{csv_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: The JSON-LD `@context` is not standard. Refer to the official @context (e.g., from the example datasets in https://github.com/mlcommons/croissant/tree/main/datasets/1.0). The different keys are: {'rai', 'examples'}\n",
      "WARNING:absl:Found the following 2 warning(s) during the validation:\n",
      "  -  [Metadata(Aricles of Indian Constitution)] Property \"http://mlcommons.org/croissant/citeAs\" is recommended, but does not exist.\n",
      "  -  [Metadata(Aricles of Indian Constitution)] Property \"https://schema.org/datePublished\" is recommended, but does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: indian_constitution.csv\n",
      "\n",
      "Using record set: indian_constitution.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.kaggle.com/api/v1/datasets/download/vyomrohila/aricles-of-indian-constitution?datasetVersionNumber=1...: 100%|██████████| 105k/105k [00:00<00:00, 224kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  indian_constitution.csv/Part+No. indian_constitution.csv/Article+No.  \\\n",
      "0                        b'Part I'                   b'\\nArticle\\n1\\n'   \n",
      "1                        b'Part I'                   b'\\nArticle\\n2\\n'   \n",
      "2                        b'Part I'                   b'\\nArticle\\n3\\n'   \n",
      "3                        b'Part I'                   b'\\nArticle\\n4\\n'   \n",
      "4                       b'Part II'                   b'\\nArticle\\n5\\n'   \n",
      "\n",
      "             indian_constitution.csv/Article+Heading  \\\n",
      "0                 b'Name and Territory of the Union'   \n",
      "1        b'Admission or establishment of new States'   \n",
      "2  b'Formation of new States and alteration of ar...   \n",
      "3  b'Laws made under articles 2 and 3 to provide ...   \n",
      "4  b'Citizenship at the commencement of the Const...   \n",
      "\n",
      "         indian_constitution.csv/Article+Description  \n",
      "0  b'\\n(1) India, that is Bharat, shall be a Unio...  \n",
      "1  b'\\nParliament may by law admit into the Union...  \n",
      "2  b'\\nParliament may by law\\xe2\\x80\\x94\\n(a) for...  \n",
      "3  b'\\n(1) Any law referred to in article 2 or ar...  \n",
      "4  b'\\nAt the commencement of this Constitution, ...  \n",
      "\n",
      "Saved dataset to 'indian_constitution.csv.csv'\n"
     ]
    }
   ],
   "source": [
    "import mlcroissant as mlc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Croissant Dataset\n",
    "croissant_dataset = mlc.Dataset('https://www.kaggle.com/datasets/vyomrohila/aricles-of-indian-constitution/croissant/download')\n",
    "\n",
    "# List all record sets\n",
    "record_sets = croissant_dataset.metadata.record_sets\n",
    "for i, rs in enumerate(record_sets):\n",
    "    print(f\"{i}: {rs.name}\")\n",
    "\n",
    "# Assuming you're using the first record set\n",
    "record_set = record_sets[0]\n",
    "print(f\"\\nUsing record set: {record_set.name}\")\n",
    "\n",
    "# Convert records to DataFrame\n",
    "records = croissant_dataset.records(record_set=record_set.uuid)\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Preview the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = f\"{record_set.name.replace(' ', '_')}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nSaved dataset to '{csv_filename}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mootcourtVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
